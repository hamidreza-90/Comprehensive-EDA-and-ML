{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "In-depth EDA and ML with Titanic .ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ079B0i6L_3"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\"1.-Introduction-1\">1. Introduction</a></span></li><li><span><a href=\"#2.-Domain-information\" data-toc-modified-id=\"2.-Domain-information-2\">2. Domain information</a></span></li><li><span><a href=\"#3.-Loading-libraries:\" data-toc-modified-id=\"3.-Loading-libraries:-3\">3. Loading libraries:</a></span></li><li><span><a href=\"#4.-Loading-data\" data-toc-modified-id=\"4.-Loading-data-4\">4. Loading data</a></span></li><li><span><a href=\"#5.-First-look:-variables,-NAs\" data-toc-modified-id=\"5.-First-look:-variables,-NAs-5\">5. First look: variables, NAs</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Variables\" data-toc-modified-id=\"5.1-Variables-5.1\">5.1 Variables</a></span></li><li><span><a href=\"#5.2-Types-of-the-variables\" data-toc-modified-id=\"5.2-Types-of-the-variables-5.2\">5.2 Types of the variables</a></span></li><li><span><a href=\"#5.3-Check-data-for-NA\" data-toc-modified-id=\"5.3-Check-data-for-NA-5.3\">5.3 Check data for NA</a></span></li></ul></li><li><span><a href=\"#6.-Exploring-the-data\" data-toc-modified-id=\"6.-Exploring-the-data-6\">6. Exploring the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.1-Survivals---target-value\" data-toc-modified-id=\"6.1-Survivals---target-value-6.1\">6.1 Survivals - target value</a></span></li><li><span><a href=\"#6.2-AGE\" data-toc-modified-id=\"6.2-AGE-6.2\">6.2 AGE</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.2.1-Age-distribution\" data-toc-modified-id=\"6.2.1-Age-distribution-6.2.1\">6.2.1 Age distribution</a></span></li><li><span><a href=\"#6.2.2-Age-by-surviving-status\" data-toc-modified-id=\"6.2.2-Age-by-surviving-status-6.2.2\">6.2.2 Age by surviving status</a></span></li><li><span><a href=\"#6.2.3-Age-by-class\" data-toc-modified-id=\"6.2.3-Age-by-class-6.2.3\">6.2.3 Age by class</a></span></li><li><span><a href=\"#6.2.4-Age-vs-class-vs-gender\" data-toc-modified-id=\"6.2.4-Age-vs-class-vs-gender-6.2.4\">6.2.4 Age vs class vs gender</a></span></li></ul></li><li><span><a href=\"#6.3-What-is-in-the-name?\" data-toc-modified-id=\"6.3-What-is-in-the-name?-6.3\">6.3 What is in the name?</a></span></li><li><span><a href=\"#6.4-Cabin\" data-toc-modified-id=\"6.4-Cabin-6.4\">6.4 Cabin</a></span></li><li><span><a href=\"#6.5-Family\" data-toc-modified-id=\"6.5-Family-6.5\">6.5 Family</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.5.1-Calculate-family-size\" data-toc-modified-id=\"6.5.1-Calculate-family-size-6.5.1\">6.5.1 Calculate family size</a></span><li><span><a href=\"#6.5.2-Family-size-and-chanses-for-surviving\" data-toc-modified-id=\"6.5.2-Family-size-and-chanses-for-surviving-6.5.2\">6.5.2 Family size and chanses for surviving</a></span></li></ul></li><li><span><a href=\"#6.6-Class\" data-toc-modified-id=\"6.6-Class-6.6\">6.6 Class</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.6.1-Passengers-by-class\" data-toc-modified-id=\"6.6.1-Passengers-by-class-6.6.1\">6.6.1 Passengers by class</a></span></li><li><span><a href=\"#6.6.2-Class-vs-surviving-status\" data-toc-modified-id=\"6.6.2-Class-vs-surviving-status-6.6.2\">6.6.2 Class vs surviving status</a></span></li><li><span><a href=\"#6.6.3-Class-vs-surviving-status-vs-gender\" data-toc-modified-id=\"6.6.3-Class-vs-surviving-status-vs-gender-6.6.3\">6.6.3 Class vs surviving status vs gender</a></span></li><li><span><a href=\"#6.6.4-Class-vs.-Gender-vs.-Age-->-Surviving-status\" data-toc-modified-id=\"6.6.4-Class-vs.-Gender-vs.-Age-->-Surviving-status-6.6.4\">6.6.4 Class vs. Gender vs. Age -&gt; Surviving status</a></span></li></ul></li><li><span><a href=\"#6.7-Gender\" data-toc-modified-id=\"6.7-Gender-6.7\">6.7 Gender</a></span></li><li><span><a href=\"#6.8-Embarked\" data-toc-modified-id=\"6.8-Embarked-6.8\">6.8 Embarked</a></span></li><li><span><a href=\"#6.9-Fare\" data-toc-modified-id=\"6.9-Fare-6.9\">6.9 Fare</a></span></li></ul></li><li><span><a href=\"#7.-Conclusion\" data-toc-modified-id=\"7.-Conclusion-7\">7. Conclusion</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PDasKod6L_6"
      },
      "source": [
        "# 1. Introduction\n",
        "The main goal of this notebook is to try to present a complete approach to data nalytics and machine learning, that goes from Exploratory Data Analysis to applying Supervised and Unsupervised learning techniques to our data. This kernel has been divided into four parts. The first part deals with the development of a baseline model. This model should allow us to quickly understand the problem and the data. Afterwards, we will go into detail. Data will be studied and enriched through exploratory data analysis and feature extraction, to improve the performance of our machine learning model. Finally, some conclusions will be drawn from this kernel and its impact in our data science journey."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VWVW4IN6L_7"
      },
      "source": [
        "# 2. Domain information\n",
        "[Titanic](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic) was a British passenger liner operated by the White Star Line. Titanic was on its way from Southampton to New York City when it sank in the North Atlantic Ocean in the early morning hours of **15 April 1912** after Titanic collided with an iceberg. The ship carried **2224** people, considering passengers and crew aboard,**1514** of them died.  \n",
        "\n",
        "Titanic carried 16 wooden lifeboats and four collapsibles, which could accommodate 1,178 people, only one-third of Titanic's total capacity (and 53% of real number of passengers). At the time, lifeboats were intended to ferry survivors from a sinking ship to a rescuing ship—not keep afloat the whole population or power them to shore. If the SS Californian would responded to Titanic's distress calls, the lifeboats may have been adequate to ferry the passengers to safety as planned, but it didn't happen and the only way to survive were to get on the lifeboat.\n",
        "\n",
        "The main question of the competition is **“what sorts of people were more likely to survive?”**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWjywxH06L_8"
      },
      "source": [
        "# 3. Loading libraries: \n",
        "\n",
        "List of libraries I am using in this kernel:\n",
        "* pandas - offers data structures and operations for manipulating numerical tables and time series. (imported as pd) [Documentation](https://pandas.pydata.org/)\n",
        "* seaborn - data visualization library based on matplotlib. [Documentation](https://seaborn.pydata.org/)\n",
        "* matplotlib.pyplot - to create some visualizations (imported as plt) [Documentation](https://matplotlib.org/tutorials/introductory/pyplot.html)\n",
        "* numpy - The fundamental package for scientific computing with Python. [Documentation](https://numpy.org/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "QeYmsaONE9gM"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIwuhwaT6L_9"
      },
      "source": [
        "# 4. Loading data\n",
        "As input information I have two CSV files:\n",
        "\n",
        "* **train.csv** - training part of the dataset, contains labels and information about passengers.\n",
        "* **test.csv** - testing part of the dataset, doesn't contain labels.\n",
        "\n",
        "At the first part of this notebook I will use all available information (train + test datasets) to perform exploratory data analysis.  \n",
        "\n",
        "1. First, load both csv files into two DataFrames, using pandas read_csv method and check the shape of the loaded data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "ogk1tVzE6L_9"
      },
      "source": [
        "# path to train dataset\n",
        "train_path = '../train.csv'\n",
        "# path to test dataset\n",
        "test_path = '../test.csv'\n",
        "\n",
        "# Read a comma-separated values (csv) file into pandas DataFrame\n",
        "train_data = pd.read_csv(train_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# shape of tha data\n",
        "print('Train shape: ', train_data.shape)\n",
        "print('Test shape: ', test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "423MSbWP6L_-"
      },
      "source": [
        "The training part contains information about 891 passengers, described by 12 variables, including 1 target variable.  \n",
        "The testing part contains 418 observations, i.e information about passengers, described by 11 variables (the test dataset doesn't contain target value.)\n",
        "\n",
        "2. Combine test and train data into one \"all_data\" DataFrame.   \n",
        "To do so, I create a sequence of DataFrame objects and use pandas concat method. Terget values of testing data in resulting dataset will be NaN.  \n",
        "Check the shape of the result DataFrame and take a look at the first 4 rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "jLoKHIPB6L_-"
      },
      "source": [
        "# create a sequence of DataFrame objects\n",
        "frames = [train_data, test_data]\n",
        "# Concatenate pandas objects along a particular axis \n",
        "all_data = pd.concat(frames, sort = False)\n",
        "# shape of the data\n",
        "print('All data shape: ', all_data.shape)\n",
        "# Show first 4 rows of the concatenated DataFrame\n",
        "all_data.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrl2t_6M6L__"
      },
      "source": [
        "Overall, we have information about 1309 passengers. I am guessing, this dataset contains data only about passengers, not crew members (we know, that Titanic carried 2224 people)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4XwAdZS6L__"
      },
      "source": [
        "# 5. First look: variables, NAs \n",
        "## 5.1 Variables\n",
        "From the data overview of the competition, we have a description of each variable:\n",
        "* PassengerId - unique identifier\n",
        "* Survived:\n",
        "        0 = No\n",
        "        1 = Yes\n",
        "* Pclass: Ticket class\n",
        "        1 = 1st, Upper\n",
        "        2 = 2nd, Middle\n",
        "        3 = 3rd, Lower\n",
        "* Name: full name with a title\n",
        "* Sex: gender\n",
        "* Age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
        "* Sibsp: Number of siblings / spouses aboard the Titanic. The dataset defines family relations in this way:\n",
        "        Sibling = brother, sister, stepbrother, stepsister\n",
        "        Spouse = husband, wife (mistresses and fiancés were ignored)\n",
        "* Parch: Number of parents / children aboard the Titanic. The dataset defines family relations in this way:\n",
        "        Parent = mother, father\n",
        "        Child = daughter, son, stepdaughter, stepson\n",
        "        Some children travelled only with a nanny, therefore parch=0 for them.\n",
        "* Ticket: Ticket number.\n",
        "* Fare: Passenger fare.\n",
        "* Cabin: Cabin number.\n",
        "* Embarked: Port of Embarkation:\n",
        "        C = Cherbourg\n",
        "        Q = Queenstown\n",
        "        S = Southampton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L30R4uJi6MAA"
      },
      "source": [
        "## 5.2 Types of the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R81S5O4u6MAB"
      },
      "source": [
        "Data types, non-null values count:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "0ka9c_mE6MAB"
      },
      "source": [
        "all_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hijAZLDp6MAB"
      },
      "source": [
        "Age and Fare are continuous numeric variables.  \n",
        "Pclass is integer, but in fact it is categorical variable, represented by 3 numbers.  \n",
        "After previous manipulations, Survived variable has type 'float', it's not correct, since it's categorical variable too, but it will not influence my EDA process, so I will let it be float for now.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S22GufBn6MAB"
      },
      "source": [
        "## 5.3 Check data for NA\n",
        "To check the dataset for NAs I am using **isna()** dataframe function, which returns a boolean same-sized object indicating if the values are NA and then I am calculating the number of True values for each variable.   \n",
        "NA values for each dataframe (train, test, all) presented in the table below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "WxOONf2L6MAC"
      },
      "source": [
        "# check data for NA values\n",
        "all_data_NA = all_data.isna().sum()\n",
        "train_NA = train_data.isna().sum()\n",
        "test_NA = test_data.isna().sum()\n",
        "\n",
        "pd.concat([train_NA, test_NA, all_data_NA], axis=1, sort = False, keys = ['Train NA', 'Test NA', 'All NA'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6nwBE2b6MAC"
      },
      "source": [
        "There are overall 263 missing Age values, 1 missing Fare, 1014 NAs in Cabin variable, and 2 in Embarked variable.  \n",
        "418 NA in Survived variable due to the absence of this information in the test dataset. I will not impute these missings in the current notebook :) So, when I will use this variable for visualization, there will be information only for the training part of the data.\n",
        "\n",
        "In this notebook I will do some missing data handling for the combined dataset. But in the second part of my work (ML solution) this should be done based on what we know only about training data, to avoid any data leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-NV2eQ56MAH"
      },
      "source": [
        "# 6. Exploring the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYj1iD6o6MAH"
      },
      "source": [
        "## 6.1 Survivals - target value\n",
        "Let's calculate and visualise distribution of our target variable - 'Survived'.  \n",
        "A countplot of seaborn module is a very useful way to show the counts of observations in each category.  \n",
        "Since we have target only for the training part, these numbers don't include all passengers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "MypCYXth6MAI"
      },
      "source": [
        "# set size of the plot\n",
        "plt.figure(figsize=(6, 4.5)) \n",
        "\n",
        "# countplot shows the counts of observations in each categorical bin using bars.\n",
        "# x - name of the categorical variable\n",
        "ax = sns.countplot(x = 'Survived', data = all_data, palette=[\"#3f3e6fd1\", \"#85c6a9\"])\n",
        "\n",
        "# set the current tick locations and labels of the x-axis.\n",
        "plt.xticks( np.arange(2), ['drowned', 'survived'] )\n",
        "# set title\n",
        "plt.title('Overall survival (training dataset)',fontsize= 14)\n",
        "# set x label\n",
        "plt.xlabel('Passenger status after the tragedy')\n",
        "# set y label\n",
        "plt.ylabel('Number of passengers')\n",
        "\n",
        "# calculate passengers for each category\n",
        "labels = (all_data['Survived'].value_counts())\n",
        "# add result numbers on barchart\n",
        "for i, v in enumerate(labels):\n",
        "    ax.text(i, v-40, str(v), horizontalalignment = 'center', size = 14, color = 'w', fontweight = 'bold')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b5YQRyji6MAI"
      },
      "source": [
        "all_data['Survived'].value_counts(normalize = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO8GIslR6MAJ"
      },
      "source": [
        "We have 891 passengers in train dataset, 549 (61,6%) of them drowned and only 342 (38,4%) survived.  \n",
        "But we know, that lifeboats could carry 53% of total passengers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm1f9-fF6MAJ"
      },
      "source": [
        "## 6.2 AGE\n",
        "What is the age of passengers, how it relates to the chances of survival, and how it changes depending on class and gender.\n",
        "### 6.2.1 Age distribution\n",
        "We have 263 missing values:  \n",
        "* 177 missing in the training dataset \n",
        "* 86 in the test dataset\n",
        "\n",
        "Overall age distribution (seaborn distplot) and descriptive statistics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "T8EVXxqP6MAK"
      },
      "source": [
        "# set plot size\n",
        "plt.figure(figsize=(15, 3))\n",
        "\n",
        "# plot a univariate distribution of Age observations \n",
        "sns.distplot(all_data[(all_data[\"Age\"] > 0)].Age, kde_kws={\"lw\": 3}, bins = 50)\n",
        "\n",
        "# set titles and labels\n",
        "plt.title('Distrubution of passengers age (all data)',fontsize= 14)\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "# clean layout\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "sIONCdqc6MAK"
      },
      "source": [
        "# Descriptive statistics include those that summarize the central tendency, \n",
        "# dispersion and shape of a dataset’s distribution, excluding NaN values.\n",
        "age_distr = pd.DataFrame(all_data['Age'].describe())\n",
        "# Transpose index and columns.\n",
        "age_distr.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgQqKiz86MAL"
      },
      "source": [
        "The distribution of Age is slightly right skewed. The Age vary from about **0.17** year to **80** years with mean = 29.88, and there don't seem to be any obvious outliers, but we will check it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7aUshxF6MAL"
      },
      "source": [
        "### 6.2.2 Age by surviving status\n",
        "Did age had a big influence on chances to survive?  \n",
        "To visualize two age distributions, grouped by surviving status I am using boxlot and stripplot showed together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "geLZRJMT6MAL"
      },
      "source": [
        "plt.figure(figsize=(15, 3))\n",
        "\n",
        "# Draw a box plot to show Age distributions with respect to survival status.\n",
        "sns.boxplot(y = 'Survived', x = 'Age', data = train_data,\n",
        "     palette=[\"#3f3e6fd1\", \"#85c6a9\"], fliersize = 0, orient = 'h')\n",
        "\n",
        "# Add a scatterplot for each category.\n",
        "sns.stripplot(y = 'Survived', x = 'Age', data = train_data,\n",
        "     linewidth = 0.6, palette=[\"#3f3e6fd1\", \"#85c6a9\"], orient = 'h')\n",
        "\n",
        "plt.yticks( np.arange(2), ['drowned', 'survived'])\n",
        "plt.title('Age distribution grouped by surviving status (train data)',fontsize= 14)\n",
        "plt.ylabel('Passenger status after the tragedy')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "pxx6di6s6MAM"
      },
      "source": [
        "# Descriptive statistics:\n",
        "pd.DataFrame(all_data.groupby('Survived')['Age'].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7DRlfs36MAM"
      },
      "source": [
        "The mean age of survived passenger is 28.34 which on 2.28 smaller than the mean age of drowned passengers (only passengers we know survived status for).   \n",
        "The minimum age of drowned passengers is 1 y.o which is very sad.  \n",
        "The maximum age of survived passenger is 80 y.o, let's check if there is no mistake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "YcckJk-U6MAM"
      },
      "source": [
        "all_data[all_data['Age'] == max(all_data['Age'] )]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3qi86Yx6MAM"
      },
      "source": [
        "Actually, Mr Algernon Henry Barkworth was born on 4 June 1864.He was 48 in 1912 and died in 1945 at 80 y.o.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "fcsMiWI-6MAM"
      },
      "source": [
        "train_data.loc[train_data['PassengerId'] == 631, 'Age'] = 48\n",
        "all_data.loc[all_data['PassengerId'] == 631, 'Age'] = 48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "uUmGfp4e6MAM"
      },
      "source": [
        "# Descriptive statistics:\n",
        "pd.DataFrame(all_data.groupby('Survived')['Age'].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_cCtTE46MAN"
      },
      "source": [
        "Let's update our description:  \n",
        "The mean age of survived passenger is 28.23 which on 2.39 smaller than the mean age of drowned passengers (only passengers we know survived status for).=\n",
        "The maximum age of survived passenger is 63 y.o.  \n",
        "It looks like there is a slightly bigger chance to survive for younger people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9qk1-906MAN"
      },
      "source": [
        "### 6.2.3 Age by class\n",
        "There I will compare three age distributions, grouped by class of the passenger.  \n",
        "As visualisations I will use 2 gaphs:\n",
        "1. boxplot+stripplot as before\n",
        "2. kdeplot, to plot age density curves for each class. This method can't handle missing values, so I filter the data before using it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "rk0NJZp56MAN"
      },
      "source": [
        "# set size\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "# set palette\n",
        "palette = sns.cubehelix_palette(5, start = 3)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x = 'Pclass', y = 'Age', data = all_data,\n",
        "     palette = palette, fliersize = 0)\n",
        "\n",
        "sns.stripplot(x = 'Pclass', y = 'Age', data = all_data,\n",
        "     linewidth = 0.6, palette = palette)\n",
        "plt.xticks( np.arange(3), ['1st class', '2nd class', '3rd class'])\n",
        "plt.title('Age distribution grouped by ticket class (all data)',fontsize= 16)\n",
        "plt.xlabel('Ticket class')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# To use kdeplot I need to create variables with filtered data for each category\n",
        "age_1_class = all_data[(all_data[\"Age\"] > 0) & \n",
        "                              (all_data[\"Pclass\"] == 1)]\n",
        "age_2_class = all_data[(all_data[\"Age\"] > 0) & \n",
        "                              (all_data[\"Pclass\"] == 2)]\n",
        "age_3_class = all_data[(all_data[\"Age\"] > 0) & \n",
        "                              (all_data[\"Pclass\"] == 3)]\n",
        "\n",
        "# Ploting the 3 variables that we create\n",
        "sns.kdeplot(age_1_class[\"Age\"], shade=True, color='#eed4d0', label = '1st class')\n",
        "sns.kdeplot(age_2_class[\"Age\"], shade=True,  color='#cda0aa', label = '2nd class')\n",
        "sns.kdeplot(age_3_class[\"Age\"], shade=True,color='#a2708e', label = '3rd class')\n",
        "plt.title('Age distribution grouped by ticket class (all data)',fontsize= 16)\n",
        "plt.xlabel('Age')\n",
        "plt.xlim(0, 90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "lP9qX5gw6MAN"
      },
      "source": [
        "# Descriptive statistics:\n",
        "pd.DataFrame(all_data.groupby('Pclass')['Age'].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0HsLUCU6MAN"
      },
      "source": [
        "1st class has wider distribution compare to 2nd and 3rd and almost symmetric.  \n",
        "Both 2nd and 3rd classes age distributions are right skewed.  \n",
        "The youngest passenger has 3rd class ticket, age = 0.17.  \n",
        "The oldest passenger has 1st class ticket, age = 76.  \n",
        "3rd class mean age = 24.8, 2nd classe average age is 29.5 and 1st class average age is 39.1.   \n",
        " \n",
        " \n",
        "Since survived passengers on average younger than drowned, does it mean, that 3rd class passengers had more chances to survive? We will discover it later.  \n",
        "\n",
        "From graphs we can see difference in age distribution between classes. So when I will do missing data imputation I will take class into account.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dt03ycO6MAO"
      },
      "source": [
        "### 6.2.4 Age vs class vs gender\n",
        "Comparison of age distribution by gender I will do separately for each class since we have such a noticeable age difference between classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "Mmmt1OKW6MAO"
      },
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "palette = \"Set3\"\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.boxplot(x = 'Sex', y = 'Age', data = age_1_class,\n",
        "     palette = palette, fliersize = 0)\n",
        "sns.stripplot(x = 'Sex', y = 'Age', data = age_1_class,\n",
        "     linewidth = 0.6, palette = palette)\n",
        "plt.title('1st class Age distribution by Sex',fontsize= 14)\n",
        "plt.ylim(-5, 80)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.boxplot(x = 'Sex', y = 'Age', data = age_2_class,\n",
        "     palette = palette, fliersize = 0)\n",
        "sns.stripplot(x = 'Sex', y = 'Age', data = age_2_class,\n",
        "     linewidth = 0.6, palette = palette)\n",
        "plt.title('2nd class Age distribution by Sex',fontsize= 14)\n",
        "plt.ylim(-5, 80)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.boxplot(x = 'Sex', y = 'Age',  data = age_3_class,\n",
        "     order = ['female', 'male'], palette = palette, fliersize = 0)\n",
        "sns.stripplot(x = 'Sex', y = 'Age', data = age_3_class,\n",
        "     order = ['female', 'male'], linewidth = 0.6, palette = palette)\n",
        "plt.title('3rd class Age distribution by Sex',fontsize= 14)\n",
        "plt.ylim(-5, 80)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "xb_inwrv6MAO"
      },
      "source": [
        "# Descriptive statistics:\n",
        "age_1_class_stat = pd.DataFrame(age_1_class.groupby('Sex')['Age'].describe())\n",
        "age_2_class_stat = pd.DataFrame(age_2_class.groupby('Sex')['Age'].describe())\n",
        "age_3_class_stat = pd.DataFrame(age_3_class.groupby('Sex')['Age'].describe())\n",
        "\n",
        "pd.concat([age_1_class_stat, age_2_class_stat, age_3_class_stat], axis=0, sort = False, keys = ['1st', '2nd', '3rd'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MFATmqL6MAO"
      },
      "source": [
        "The oldest and the youngest passengers are female.  \n",
        "In each class the average Age of female are slightly less than the average Age of male passengers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO6_EYYL6MAO"
      },
      "source": [
        "## 6.3 What is in the name? \n",
        "Each passenger Name value contains the title of the passenger which we can extract and discover.  \n",
        "To create new variable \"Title\":  \n",
        "1. I am using method 'split' by comma to divide Name in two parts and save the second part\n",
        "2. I am splitting saved part by dot and save first part of the result\n",
        "3. To remove spaces around the title I am using 'split' method\n",
        "\n",
        "To visualize, how many passengers hold each title, I chose countplot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "zkqGJzD86MAP"
      },
      "source": [
        "all_data['Title'] = all_data['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "ax = sns.countplot( x = 'Title', data = all_data, palette = \"hls\", order = all_data['Title'].value_counts().index)\n",
        "_ = plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light'  \n",
        ")\n",
        "\n",
        "plt.title('Passengers distribution by titles',fontsize= 14)\n",
        "plt.ylabel('Number of passengers')\n",
        "\n",
        "# calculate passengers for each category\n",
        "labels = (all_data['Title'].value_counts())\n",
        "# add result numbers on barchart\n",
        "for i, v in enumerate(labels):\n",
        "    ax.text(i, v+10, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n",
        "    \n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa_PM4qR6MAP"
      },
      "source": [
        "The most frequent title among passangers is Mister (Mr.) - general title or respect of an adult male. The second title by its frequency is Miss (unmarried woman), the third - Mrs. (married woman).  \n",
        "Other titles are less frequent, I will discover if I can combine them into particular groups. I am going to use titles as a fiture, but if they split the data too much, leaving just a few observations in each group, it can lead to overfitting. And for a general understanding of the data, it will be more convenient to put titles in clearer groups.\n",
        "\n",
        "* Master -  By the late 19th century, etiquette dictated that men be addressed as Mister, and boys as Master.\n",
        "* Mme -  Madame. a French title of respect equivalent to “Mrs.”, used alone or prefixed to a woman's married name or title. I will add to \"Mrs\".\n",
        "* Mlle - Mademoiselle is a French courtesy title, abbreviated Mlle, traditionally given to an unmarried woman. The equivalent in English is \"Miss\". I will add to \"Miss\" group.\n",
        "* Dr. - Doctor is an academic title\n",
        "* Rev. - Reverend, is usually a courtesy title for Protestant Christian ministers or pastors.\n",
        "\n",
        "\"Military\" group of titles:\n",
        "* Capt. -Captain is a title for the commander of a military unit\n",
        "* Major is a military rank of commissioned officer status\n",
        "* Col. - The honorary title of Colonel is conferred by several states in the US and certain military units of the Commonwealth of Nations\n",
        "\n",
        "\"Honor\"  group of titles:\n",
        "* Sir -  is a formal English honorific address for men. Sir is used for men titled knights i.e. of orders of chivalry, and later also to baronets, and other offices. \n",
        "* the Countess - is a historical title of nobility\n",
        "* Lady - a formal title in the United Kingdom. A woman with a title of nobility or honorary \n",
        "* Jonkheer - is an honorific in the Low Countries denoting the lowest rank within the nobility. \n",
        "* Don - is an honorific prefix primarily used in Spain and the former Spanish Empire, Italy, Portugal, the Philippines, Latin America, Croatia, and Goa. (male)\n",
        "* Dona - Feminine form for don (honorific) a Spanish, Portuguese, southern Italian, and Filipino title, given as a mark of respect\n",
        "\n",
        "\n",
        "Not sure about the title Ms,we have only 2 passengers with this title, I will convert it to Miss.\n",
        "\n",
        "I created a dictionary of titles and I am using method \"map\" to create variable \"Title_category\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "ycJPLHZT6MAP"
      },
      "source": [
        "all_data[all_data['Title']=='Ms']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "L0WvhLw06MAQ"
      },
      "source": [
        "title_dict = {  'Mr':     'Mr',\n",
        "                'Mrs':    'Mrs',\n",
        "                'Miss':   'Miss',\n",
        "                'Master': 'Master',\n",
        "              \n",
        "                'Ms':     'Miss',\n",
        "                'Mme':    'Mrs',\n",
        "                'Mlle':   'Miss',\n",
        "\n",
        "                'Capt':   'military',\n",
        "                'Col':    'military',\n",
        "                'Major':  'military',\n",
        "\n",
        "                'Dr':     'Dr',\n",
        "                'Rev':    'Rev',\n",
        "                  \n",
        "                'Sir':    'honor',\n",
        "                'the Countess': 'honor',\n",
        "                'Lady':   'honor',\n",
        "                'Jonkheer': 'honor',\n",
        "                'Don':    'honor',\n",
        "                'Dona':   'honor' }\n",
        "\n",
        "# map titles to category\n",
        "all_data['Title_category'] = all_data['Title'].map(title_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "SfF8Us7y6MAQ"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax = sns.countplot(x = 'Title_category', data = all_data, palette = \"hls\", order = all_data['Title_category'].value_counts().index)\n",
        "_ = plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light'  \n",
        ")\n",
        "plt.title('Passengers distribution by titles',fontsize= 12)\n",
        "plt.ylabel('Number of passengers')\n",
        "\n",
        "# calculate passengers for each category\n",
        "labels = (all_data['Title_category'].value_counts())\n",
        "# add result numbers on barchart\n",
        "for i, v in enumerate(labels):\n",
        "    ax.text(i, v+10, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n",
        "    \n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "surv_by_title_cat = all_data.groupby('Title_category')['Survived'].value_counts(normalize = True).unstack()\n",
        "surv_by_title_cat = surv_by_title_cat.sort_values(by=1, ascending = False)\n",
        "surv_by_title_cat.plot(kind='bar', stacked='True', color=[\"#3f3e6fd1\", \"#85c6a9\"], ax = ax2)\n",
        "\n",
        "plt.legend( ( 'Drowned', 'Survived'), loc=(1.04,0))\n",
        "_ = plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light'  \n",
        ")\n",
        "\n",
        "\n",
        "plt.title('Proportion of survived/drowned by titles (train data)',fontsize= 12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfrhC6Ri6MAQ"
      },
      "source": [
        "* The smallest group is \"honor\", passengers with royal-kind titles.  \n",
        "\n",
        "Training data:\n",
        "* The biggiest proportion of survivals is in \"Mrs\" group - married woman.   \n",
        "* More than 80% drowned in \"Mr.\" group.  \n",
        "* Nobody survived among Reverend group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "ypEHTqV06MAR"
      },
      "source": [
        "category_survived = sns.catplot(x=\"Title_category\",  col=\"Survived\",\n",
        "                data = all_data, kind=\"count\",\n",
        "                height=4, aspect=.7)\n",
        "\n",
        "category_survived.set_xticklabels(rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNP3o3O-6MAR"
      },
      "source": [
        "If we consider the survivors not by percentage within each group but by comparing the number of survivors between groups, then \"Miss\" title category is the luckiest one. The \"Mr\" category lost the biggest number of passengers.\n",
        "\n",
        "Let's also visualize, how Title categories and ticket classes are related:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "_tTFIIAn6MAR"
      },
      "source": [
        "class_by_title_cat = all_data.groupby('Title_category')['Pclass'].value_counts(normalize = True)\n",
        "class_by_title_cat = class_by_title_cat.unstack().sort_values(by = 1, ascending = False)\n",
        "class_by_title_cat.plot(kind='bar', stacked='True', color = ['#eed4d0', '#cda0aa', '#a2708e'])\n",
        "plt.legend(loc=(1.04,0))\n",
        "_ = plt.xticks(\n",
        "    rotation = 45, \n",
        "    horizontalalignment = 'right',\n",
        "    fontweight = 'light'  \n",
        ")\n",
        "\n",
        "\n",
        "plt.title('Proportion of 1st/2nd/3rd ticket class in each title category',fontsize= 14)\n",
        "plt.xlabel('Category of the Title')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q1lfxTY6MAS"
      },
      "source": [
        "* All honor and military titles occupied the 1st class.  \n",
        "* All Reverends occupied 2nd class.\n",
        "* The biggest percent of the 3rd class is in the Master category.\n",
        "\n",
        "For sure, there is a relationship between variables, and survival was influenced not only by the title itself but by a combination of factors that are to some extent interrelated. How class could relate on surviving? Let's go further and discover."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBKUZtbA6MAS"
      },
      "source": [
        "## 6.4 Cabin\n",
        "\n",
        "From the number of the cabin we can extract first letter, which will tell us about placement of the cabin on the ship! And it seems to me as a very important knowlege:   \n",
        "* How close cabin located to the lifeboats\n",
        "* How far from the most damaged parts of the ship\n",
        "* How close to people who have information about what is happening and how to act\n",
        "* How many obstacles passenger had in the way to the lifeboat\n",
        "\n",
        "I found some description of each Titanic deck:\n",
        "\n",
        "There were 8 decks: the upperdeck - for lifeboats, other 7 were under it and had letter symbols:\n",
        "* **A**: it did not run the entire length of the vessel (i.e. it did not reach from the stern to the bow of the vessel), and was intended for passengers of the **1st class**.\n",
        "* **B**: it did not run the entire length of the ship (it was interrupted by 37 meters above the C deck, and served as a place for anchors in the front).\n",
        "* **C**: in the front part of the galley, dining room for the crew, as well as a walking area for passengers of the 3rd class.\n",
        "* **D**: a walking area for passengers .\n",
        "* **E**: cabins of the **1st and 2nd class**.\n",
        "* **F**: part of the passenger cabins of the **2nd class**, most of the cabins of the **3rd class**.\n",
        "* **G**: did not run the entire length of the ship, the boiler rooms were located in the center.\n",
        "* **T** - boat deck ?\n",
        "\n",
        "To the passengers without deck information I will imput U letter (as unknown)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "Yt1vm2kO6MAS"
      },
      "source": [
        "all_data['deck'] = all_data['Cabin'].str.split('', expand = True)[1]\n",
        "all_data.loc[all_data['deck'].isna(), 'deck'] = 'U'\n",
        "print('Unique deck letters from the cabin numbers:', all_data['deck'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "kawyGzVm6MAT"
      },
      "source": [
        "fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "ax1 = fig.add_subplot(131)\n",
        "sns.countplot(x = 'deck', data = all_data, palette = \"hls\", order = all_data['deck'].value_counts().index, ax = ax1)\n",
        "plt.title('Passengers distribution by deck',fontsize= 16)\n",
        "plt.ylabel('Number of passengers')\n",
        "\n",
        "ax2 = fig.add_subplot(132)\n",
        "deck_by_class = all_data.groupby('deck')['Pclass'].value_counts(normalize = True).unstack()\n",
        "deck_by_class.plot(kind='bar', stacked='True',color = ['#eed4d0', '#cda0aa', '#a2708e'], ax = ax2)\n",
        "plt.legend(('1st class', '2nd class', '3rd class'), loc=(1.04,0))\n",
        "plt.title('Proportion of classes on each deck',fontsize= 16)\n",
        "plt.xticks(rotation = False)\n",
        "\n",
        "ax3 = fig.add_subplot(133)\n",
        "deck_by_survived = all_data.groupby('deck')['Survived'].value_counts(normalize = True).unstack()\n",
        "deck_by_survived = deck_by_survived.sort_values(by = 1, ascending = False)\n",
        "deck_by_survived.plot(kind='bar', stacked='True', color=[\"#3f3e6fd1\", \"#85c6a9\"], ax = ax3)\n",
        "plt.title('Proportion of survived/drowned passengers by deck',fontsize= 16)\n",
        "plt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\n",
        "plt.xticks(rotation = False)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soWGeD676MAT"
      },
      "source": [
        "Most passengers don't have cabin numbers ('U').  \n",
        "The largest part of passengers with known cabin numbers were located on the  'C' deck and had 1st class ticket. 'C' deck is fifth by a percentage of the survivor.  \n",
        "The largest surviving rate (among passengers with known cabin numbers in training dataset) had passengers from deck 'D'.  \n",
        "Deck A was the closest to the deck with lifeboats, but it is the last in surviving rate (except unknown and T deck). How did it happen?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "XvY7ySnS6MAT"
      },
      "source": [
        "all_data[(all_data['deck']=='A') & (all_data['Survived']==0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTvs7W406MAT"
      },
      "source": [
        "I was curious, so I read a bit about some of these passengers:  \n",
        "[John Hugo Ross](https://www.encyclopedia-titanica.org/titanic-victim/john-hugo-ross.html) When he boarded on 10 April 1912, he was so ill from dysentery he had to be carried to his cabin on a stretcher. When Ross was told the ship had struck an iceberg and that he should get dressed, Ross refused to believe the trouble serious. \"Is that all,?\" he told Peuchen. \"It will take more than an iceberg to get me off this ship.\" Presumably, Ross drowned in his bed.  \n",
        "\n",
        "[Andrews, Mr. Thomas Jr](https://en.wikipedia.org/wiki/Thomas_Andrews) was a managing director of H&W (built the Titanic) in charge of designing and was familiar with every detail of the construction of the firm's ships. He helped to evacuate people.\n",
        "\n",
        "[Roebling, Mr. Washington Augustus II ](https://www.encyclopedia-titanica.org/titanic-victim/washington-roebling.html) helped to evacuate people as well.\n",
        "\n",
        "It is obvious that there is no algorithm that can predict the survival rate by 100 percent based on the factors of the passenger's location on the ship or his age, since the human factor and the unpredicted emergensy were involved in the rescue process.\n",
        "\n",
        "For training process it will be better to include passenger from T deck to the A deck group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLeM0cSw6MAU"
      },
      "source": [
        "## 6.5 Family\n",
        "Does the size of the family on Board together affect the chances of surviving a disaster? Does having children increase the chance of getting into a boat, or is it easier to survive being single?  \n",
        "I calculate the family size, by summarizing the number of siblings with the parch number  plus 1 (passenger himself).  \n",
        "Family size = sib + parch + 1\n",
        "\n",
        "### 6.5.1 Calculate family size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "1ZwOa2ab6MAU",
        "outputId": "40f64325-6085-4c45-b5b5-9882533b76b9"
      },
      "source": [
        "all_data['Family_size'] = all_data['SibSp'] + all_data['Parch'] + 1\n",
        "family_size = all_data['Family_size'].value_counts()\n",
        "print('Family size and number of passengers:')\n",
        "print(family_size)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cb18edb7a0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Family_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SibSp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Parch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfamily_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Family_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Family size and number of passengers:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfamily_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz5H-BaZ6MAV"
      },
      "source": [
        "Looks strange that there are 16 passengers with family size of 7, for example. Let's check!  \n",
        "Also, I will add a surname variable, by extraxting first word ow the name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "4T6zMdGb6MAV"
      },
      "source": [
        "all_data['Surname'] = all_data['Name'].str.split(',', expand = True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_cLttF6MAV"
      },
      "source": [
        "#### size 7\n",
        "1. Group people with family size = 7 by Surname  \n",
        "We have 9 Andersons, who have family size of 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "oD35yKEs6MAV"
      },
      "source": [
        "all_data[all_data['Family_size'] == 7]['Surname'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "XuH-Rrmp6MAW"
      },
      "source": [
        "all_data[(all_data['Family_size'] == 7) & (all_data['Surname']=='Andersson')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fN-RFxo6MAW"
      },
      "source": [
        "2. Let's group Andersons with 7-size family by ticket number.  \n",
        "There are  7 of them used the same ticket and travelled together. 5 children (each of them has 4 siblings) and 2 parents.\n",
        "To passengers used separated tickets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "TmktT_046MAW"
      },
      "source": [
        "all_data[(all_data['Family_size'] == 7) & (all_data['Surname']=='Andersson')].Ticket.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "gcwrt43-6MAW"
      },
      "source": [
        "all_data[(all_data['Ticket'] == '3101281') | (all_data['Ticket'] == '347091')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqjq_KKq6MAX"
      },
      "source": [
        "Looks like they actually traveled alone, I will correct that data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "ZqgJk9nT6MAX"
      },
      "source": [
        "all_data.loc[all_data['PassengerId'] == 69, ['SibSp', 'Parch', 'Family_size']] = [0,0,1]\n",
        "all_data.loc[all_data['PassengerId'] == 1106, ['SibSp', 'Parch', 'Family_size']] = [0,0,1]\n",
        "all_data[(all_data['Ticket'] == '3101281') | (all_data['Ticket'] == '347091')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC2Ttq8V6MAX"
      },
      "source": [
        "#### size 5\n",
        "There are some inconsistencies in other categories, with fewer relatives.  \n",
        "Let's check people with 5-size family and group them by Surname:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "1g0nDJDJ6MAX"
      },
      "source": [
        "all_data[all_data['Family_size'] == 5]['Surname'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "5btOykg16MAY"
      },
      "source": [
        "all_data[(all_data['Surname'] == 'Kink-Heilmann')&(all_data['Family_size'] == 5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlw3OPeD6MAY"
      },
      "source": [
        "Kink-Heilmann, Mr. Anton\t had 2 other siblings on the ship unlike his wife, for whom these relatives do not fit the description of relatives in the data set. We will assume that all other \"mismatches\" in the groups are similar to this. Since I plan to group the size of families into groups, this will eliminate possible inconsistencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPftOvLD6MAY"
      },
      "source": [
        "### 6.5.2 Family size and chanses for surviving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-output": false,
        "_kg_hide-input": true,
        "id": "NcWeeiwg6MAY"
      },
      "source": [
        "fig = plt.figure(figsize = (12,4))\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax = sns.countplot(all_data['Family_size'], ax = ax1)\n",
        "\n",
        "# calculate passengers for each category\n",
        "labels = (all_data['Family_size'].value_counts())\n",
        "# add result numbers on barchart\n",
        "for i, v in enumerate(labels):\n",
        "    ax.text(i, v+6, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n",
        "    \n",
        "plt.title('Passengers distribution by family size')\n",
        "plt.ylabel('Number of passengers')\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "d = all_data.groupby('Family_size')['Survived'].value_counts(normalize = True).unstack()\n",
        "d.plot(kind='bar', color=[\"#3f3e6fd1\", \"#85c6a9\"], stacked='True', ax = ax2)\n",
        "plt.title('Proportion of survived/drowned passengers by family size (train data)')\n",
        "plt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\n",
        "plt.xticks(rotation = False)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1BIFu2m6MAY"
      },
      "source": [
        "* There were two large families with sizes 8 and 11 and all their members from the training dataset are drowned.  \n",
        "* Most of the passengers were traveling alone, percent of survivals not very large.  \n",
        "* The biggest proportion of survived passengers in group of people who had 4 family members on board. \n",
        "\n",
        "We can observe that the percentage of survivors in people who have a family of 2, 3, 4 people is greater than in singles, then the percentage of survivors decreases as the family size increases.  \n",
        "I will create 'Family_size_group' variable with four categories: \n",
        "* single\n",
        "* usual (sizes 2, 3, 4)\n",
        "* big (5, 6, 7)\n",
        "* and large (all bigger then 7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "OhZno3286MAZ"
      },
      "source": [
        "all_data['Family_size_group'] = all_data['Family_size'].map(lambda x: 'f_single' if x == 1 \n",
        "                                                            else ('f_usual' if 5 > x >= 2 \n",
        "                                                                  else ('f_big' if 8 > x >= 5 \n",
        "                                                                       else 'f_large' )\n",
        "                                                                 ))                                                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "SFWuiIpR6MAZ"
      },
      "source": [
        "fig = plt.figure(figsize = (14,5))\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "d = all_data.groupby('Family_size_group')['Survived'].value_counts(normalize = True).unstack()\n",
        "d = d.sort_values(by = 1, ascending = False)\n",
        "d.plot(kind='bar', stacked='True', color = [\"#3f3e6fd1\", \"#85c6a9\"], ax = ax1)\n",
        "plt.title('Proportion of survived/drowned passengers by family size (training data)')\n",
        "plt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\n",
        "_ = plt.xticks(rotation=False)\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "d2 = all_data.groupby('Family_size_group')['Pclass'].value_counts(normalize = True).unstack()\n",
        "d2 = d2.sort_values(by = 1, ascending = False)\n",
        "d2.plot(kind='bar', stacked='True', color = ['#eed4d0', '#cda0aa', '#a2708e'], ax = ax2)\n",
        "plt.legend(('1st class', '2nd class', '3rd class'), loc=(1.04,0))\n",
        "plt.title('Proportion of 1st/2nd/3rd ticket class in family group size')\n",
        "_ = plt.xticks(rotation=False)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqPVMCa76MAZ"
      },
      "source": [
        "Large families are all from 3rd class and no one from the training part of the dataset is survived.  \n",
        "The biggest proportion of the 1st class in the usual size of the family and the proportion of survivors in the usual family is the biggest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV2vjIiC6MAZ"
      },
      "source": [
        "## 6.6 Class\n",
        "We have made a lot of assumptions about the survival rate depending on the classes. Let's now look closely at this variable.\n",
        "\n",
        "### 6.6.1 Passengers by class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "WLZ6SVYX6MAZ"
      },
      "source": [
        "ax = sns.countplot(all_data['Pclass'], palette = ['#eed4d0', '#cda0aa', '#a2708e'])\n",
        "# calculate passengers for each category\n",
        "labels = (all_data['Pclass'].value_counts(sort = False))\n",
        "# add result numbers on barchart\n",
        "for i, v in enumerate(labels):\n",
        "    ax.text(i, v+2, str(v), horizontalalignment = 'center', size = 12, color = 'black', fontweight = 'bold')\n",
        "    \n",
        "    \n",
        "plt.title('Passengers distribution by family size')\n",
        "plt.ylabel('Number of passengers')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErDSRy1F6MAa"
      },
      "source": [
        "Most of the Titanic's passengers were traveling third class (709).  \n",
        "The second class is the smallest in terms of the number of passengers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-s5YMzN6MAa"
      },
      "source": [
        "### 6.6.2 Class vs surviving status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "ZIbTeQxR6MAa"
      },
      "source": [
        "fig = plt.figure(figsize=(14, 5))\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "sns.countplot(x = 'Pclass', hue = 'Survived', data = all_data, palette=[\"#3f3e6fd1\", \"#85c6a9\"], ax = ax1)\n",
        "plt.title('Number of survived/drowned passengers by class (train data)')\n",
        "plt.ylabel('Number of passengers')\n",
        "plt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\n",
        "_ = plt.xticks(rotation=False)\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "d = all_data.groupby('Pclass')['Survived'].value_counts(normalize = True).unstack()\n",
        "d.plot(kind='bar', stacked='True', ax = ax2, color =[\"#3f3e6fd1\", \"#85c6a9\"])\n",
        "plt.title('Proportion of survived/drowned passengers by class (train data)')\n",
        "plt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\n",
        "_ = plt.xticks(rotation=False)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2CxEet6MAa"
      },
      "source": [
        "Despite the previously identified prerequisites (on average, older people are more likely to die, and in the first class, the average age is higher than in other classes. Also, passengers on deck A, which consists of 100% first class, have a large proportion of drowned passengers), the first class has the largest number of survivors and the proportion of survivors within the class is the largest.  \n",
        "Third-class tickets had the highest number of drowned passengers, and most of the third-class passengers drowned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAKphzOT6MAa"
      },
      "source": [
        "### 6.6.3 Class vs surviving status vs gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "kL8cGonj6MAa"
      },
      "source": [
        "sns.catplot(x = 'Pclass', hue = 'Survived', col = 'Sex', kind = 'count', data = all_data , palette=[\"#3f3e6fd1\", \"#85c6a9\"])\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFleKRsN6MAb"
      },
      "source": [
        "However, most of the male passengers of the first class drowned, and the female almost all survived. In the third grade, half of the female survived."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy-ax0OE6MAb"
      },
      "source": [
        "### 6.6.4 Class vs. Gender vs. Age -> Surviving status \n",
        "For better understanding how the combination of some factors influence on chances to survive, lets break passengers into 18 imaginary groups separated by:\n",
        "* **Class** (1 / 2 / 3)\n",
        "* **Gender** (male / female)\n",
        "* **Age** ( <16 / 16-40 / 40<)  \n",
        "\n",
        "To do so I will create 6 stripplots (3 for male, 3 for female), with values grouped by Surviving status and add background color to separate age groups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "jgK6FPBK6MAb"
      },
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "palette=[\"#3f3e6fd1\", \"#85c6a9\"]\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.stripplot(x = 'Survived', y = 'Age', data = age_1_class[age_1_class['Sex']=='male'],\n",
        "     linewidth = 0.9, palette = palette)\n",
        "plt.axhspan(0, 16, color = \"#e1f3f6\")\n",
        "plt.axhspan(16, 40, color = \"#bde6dd\")\n",
        "plt.axhspan(40, 80, color = \"#83ceb9\")\n",
        "plt.title('Age distribution (males, 1st class)',fontsize= 14)\n",
        "plt.xticks( np.arange(2), ['drowned', 'survived'])\n",
        "plt.ylim(0, 80)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "sns.stripplot(x = 'Survived', y = 'Age', data = age_2_class[age_2_class['Sex']=='male'],\n",
        "     linewidth = 0.9, palette = palette)\n",
        "plt.axhspan(0, 16, color = \"#e1f3f6\")\n",
        "plt.axhspan(16, 40, color = \"#bde6dd\")\n",
        "plt.axhspan(40, 80, color = \"#83ceb9\")\n",
        "plt.title('Age distribution (males, 2nd class)',fontsize= 14)\n",
        "plt.xticks( np.arange(2), ['drowned', 'survived'])\n",
        "plt.ylim(0, 80)\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "sns.stripplot(x = 'Survived', y = 'Age', data = age_3_class[age_3_class['Sex']=='male'],\n",
        "              linewidth = 0.9, palette = palette)\n",
        "plt.axhspan(0, 16, color = \"#e1f3f6\")\n",
        "plt.axhspan(16, 40, color = \"#bde6dd\")\n",
        "plt.axhspan(40, 80, color = \"#83ceb9\")\n",
        "plt.title('Age distribution (males, 3rd class)',fontsize= 14)\n",
        "plt.xticks( np.arange(2), ['drowned', 'survived'])\n",
        "plt.ylim(0, 80)\n",
        "\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "sns.stripplot(x = 'Survived', y = 'Age', data = age_1_class[age_1_class['Sex']=='female'],\n",
        "     linewidth = 0.9, palette = palette)\n",
        "plt.axhspan(0, 16, color = \"#ffff9978\")\n",
        "plt.axhspan(16, 40, color = \"#ffff97bf\")\n",
        "plt.axhspan(40, 80, color = \"#ffed97bf\")\n",
        "plt.title('Age distribution (females, 1st class)',fontsize= 14)\n",
        "plt.xticks( np.arange(2), ['drowned', 'survived'])\n",
        "plt.ylim(0, 80)\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "sns.stripplot(x = 'Survived', y = 'Age', data = age_2_class[age_2_class['Sex']=='female'],\n",
        "     linewidth = 0.9, palette = palette)\n",
        "plt.axhspan(0, 16, color = \"#ffff9978\")\n",
        "plt.axhspan(16, 40, color = \"#ffff97bf\")\n",
        "plt.axhspan(40, 80, color = \"#ffed97bf\")\n",
        "plt.title('Age distribution (females, 2nd class)',fontsize= 14)\n",
        "plt.xticks( np.arange(2), ['drowned', 'survived'])\n",
        "plt.ylim(0, 80)\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "sns.stripplot(x = 'Survived', y = 'Age', data = age_3_class[age_3_class['Sex']=='female'],\n",
        "              linewidth = 0.9, palette = palette)\n",
        "plt.axhspan(0, 16, color = \"#ffff9978\")\n",
        "plt.axhspan(16, 40, color = \"#ffff97bf\")\n",
        "plt.axhspan(40, 80, color = \"#ffed97bf\")\n",
        "plt.title('Age distribution (females, 3rd class)',fontsize= 14)\n",
        "plt.xticks( np.arange(2), ['drowned', 'survived'])\n",
        "plt.ylim(0, 80)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gfxh31u6MAb"
      },
      "source": [
        "From these graphs (training data only) we can see, that \n",
        "* only one kid (<16) from 1st and 2nd classes are drowned (female from the 1st class).\n",
        "* but children from the 3rd class were not so lucky, looks like chances to survive for passengers <16 were 50/50 for male and female.\n",
        "* most females from 1st and 2nd class are survived, without much difference in Age.\n",
        "* females from 3rd class in Age group 40+ drowned except one.\n",
        "* similar picture for males in 2nd and 3rd classes in the Age group 40+: only 2 from each class are survived.\n",
        "* for 40+ males from 1st class situation were slightly different, there are more survived passengers.\n",
        "* the largest \"accumulation\" of drowned passengers is observed in the Age group 16-40 males, 3rd class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHb7n0FK6MAb"
      },
      "source": [
        "## 6.7 Gender\n",
        "Let's discover gender a little bit more:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "lBnOl8rL6MAb"
      },
      "source": [
        "plt.figure(figsize = (15,4))\n",
        "\n",
        "plt.subplot (1,3,1)\n",
        "ax = sns.countplot(all_data['Sex'], palette=\"Set3\")\n",
        "plt.title('Number of passengers by Sex')\n",
        "plt.ylabel('Number of passengers')\n",
        "\n",
        "# calculate passengers for each category\n",
        "labels = (all_data['Sex'].value_counts())\n",
        "# add result numbers on barchart\n",
        "for i, v in enumerate(labels):\n",
        "    ax.text(i, v+10, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n",
        "    \n",
        "\n",
        "plt.subplot (1,3,2)\n",
        "sns.countplot( x = 'Pclass', data = all_data, hue = 'Sex', palette=\"Set3\")\n",
        "plt.title('Number of male/female passengers by class')\n",
        "plt.ylabel('Number of passengers')\n",
        "plt.legend( loc=(1.04,0))\n",
        "\n",
        "plt.subplot (1,3,3)\n",
        "sns.countplot( x = 'Family_size_group', data = all_data, hue = 'Sex', \n",
        "              order = all_data['Family_size_group'].value_counts().index , palette=\"Set3\")\n",
        "plt.title('Number of male/female passengers by family size')\n",
        "plt.ylabel('Number of passengers')\n",
        "plt.legend( loc=(1.04,0))\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_LAPNVp6MAc"
      },
      "source": [
        "There were overall more males than females on board, it is fair for each ticket class, but in the 3rd class number of males more than twice bigger than females.  \n",
        "Almost 600 male passengers traveled without family members and only about 200 females, but in usual and big families there were slightly more female passengers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6B-Q1Av6MAc"
      },
      "source": [
        "## 6.8 Embarked\n",
        "Titanic had 3 embarkation points before the ship started its route to New York:\n",
        "* Southampton\n",
        "* Cherbourg\n",
        "* Queenstown  \n",
        "\n",
        "Some passengers could leave Titanic in Cherbourg or Queenstown and avoid catastrophe. Also, the point of embarkation could have an influence on ticket fare and location on the ship.  \n",
        "\n",
        "Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "cMXrqXqC6MAc"
      },
      "source": [
        "fig = plt.figure(figsize = (15,4))\n",
        "\n",
        "ax1 = fig.add_subplot(131)\n",
        "palette = sns.cubehelix_palette(5, start = 2)\n",
        "ax = sns.countplot(all_data['Embarked'], palette = palette, order = ['C', 'Q', 'S'], ax = ax1)\n",
        "plt.title('Number of passengers by Embarked')\n",
        "plt.ylabel('Number of passengers')\n",
        "\n",
        "# calculate passengers for each category\n",
        "labels = (all_data['Embarked'].value_counts())\n",
        "labels = labels.sort_index()\n",
        "# add result numbers on barchart\n",
        "for i, v in enumerate(labels):\n",
        "    ax.text(i, v+10, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n",
        "    \n",
        "\n",
        "ax2 = fig.add_subplot(132)\n",
        "surv_by_emb = all_data.groupby('Embarked')['Survived'].value_counts(normalize = True)\n",
        "surv_by_emb = surv_by_emb.unstack().sort_index()\n",
        "surv_by_emb.plot(kind='bar', stacked='True', color=[\"#3f3e6fd1\", \"#85c6a9\"], ax = ax2)\n",
        "plt.title('Proportion of survived/drowned passengers by Embarked (train data)')\n",
        "plt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\n",
        "_ = plt.xticks(rotation=False)\n",
        "\n",
        "\n",
        "ax3 = fig.add_subplot(133)\n",
        "class_by_emb = all_data.groupby('Embarked')['Pclass'].value_counts(normalize = True)\n",
        "class_by_emb = class_by_emb.unstack().sort_index()\n",
        "class_by_emb.plot(kind='bar', stacked='True', color = ['#eed4d0', '#cda0aa', '#a2708e'], ax = ax3)\n",
        "plt.legend(('1st class', '2nd class', '3rd class'), loc=(1.04,0))\n",
        "plt.title('Proportion of clases by Embarked')\n",
        "_ = plt.xticks(rotation=False)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjTZStAT6MAc"
      },
      "source": [
        "* Most number of passengers (914) were embarked in Southampton. Also Southampton has the biggiest proportion of drowned passengers.\n",
        "* 270 passengers emarked in Cherbourg and more than 50% of them survived (in the training dataset). \n",
        "* 123 of passengers embarked in Queenstown, the vast majority of them are 3rd class passengers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "A3F8sbpc6MAc"
      },
      "source": [
        "\n",
        "sns.catplot(x=\"Embarked\", y=\"Fare\", kind=\"violin\", inner=None,\n",
        "            data=all_data, height = 6, palette = palette, order = ['C', 'Q', 'S'])\n",
        "plt.title('Distribution of Fare by Embarked')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "JEaDdrRs6MAc"
      },
      "source": [
        "# Descriptive statistics:\n",
        "pd.DataFrame(all_data.groupby('Embarked')['Fare'].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWMU3rhi6MAd"
      },
      "source": [
        "* The wider fare distribution among passengers who embarked in Cherbourg. It makes scence - many first-class passengers boarded the ship here, but the share of third-class passengers is quite significant.\n",
        "* The smallest variation in the price of passengers who boarded in q. Also, the average price of these passengers is the smallest, I think this is due to the fact that the path was supposed to be the shortest + almost all third-class passengers.\n",
        "\n",
        "Let's check NA values of Embarked variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "IhZkXW-E6MAd"
      },
      "source": [
        "train_data[train_data['Embarked'].isna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsrS24PJ6MAd"
      },
      "source": [
        "These two passengers traveled together (same ticket number). To impute missing values, we can use mode value for passengers with the closest fare value and Pclass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ_a5scb6MAd"
      },
      "source": [
        "## 6.9 Fare\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "am67sF_66MAd"
      },
      "source": [
        "sns.catplot(x=\"Pclass\", y=\"Fare\", kind=\"swarm\", data=all_data, palette=sns.cubehelix_palette(5, start = 3), height = 6)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9luvm0B6MAe"
      },
      "source": [
        "We can observe that the distribution of prices for the second and third class is very similar. The distribution of first-class prices is very different, has a larger spread, and on average prices are higher. \n",
        "\n",
        "Let's add colours to our points to indicate surviving status of passenger (there will be only data from training part of the dataset):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "0GGlFclo6MAe"
      },
      "source": [
        "sns.catplot(x=\"Pclass\", y=\"Fare\",  hue = \"Survived\", kind=\"swarm\", data=all_data, \n",
        "                                    palette=[\"#3f3e6fd1\", \"#85c6a9\"], height = 6)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyNojUF36MAe"
      },
      "source": [
        "Looks like the bigger passenger paid, the more chances to survive he had.  \n",
        "What about zero fare in the first class? Is it a mistake?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "5CCLr-Mn6MAe"
      },
      "source": [
        "all_data[all_data['Fare'] == min(all_data['Fare'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6IBPAMR6MAe"
      },
      "source": [
        "Some of the passengers have \"Line\" tickets, perhaps they were somehow involved in the Titanic, but were not the ship's crew. I don't think we should change these prices, but add an additional feature for these passengers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EzWCa3I6MAe"
      },
      "source": [
        "# 7. EDA Conclusion\n",
        "We know that there were not enough boats on Board the Titanic for all passengers to be able to evacuate. After studying the information about the passengers, we can make some assumptions about who had a better chance of survival in a shipwreck situation as well as General observations about passengers.\n",
        "\n",
        "* There are 891 passengers in the train dataset, 549 (61,6%) of them drowned and only 342 (38,4%) survived. But we know, that lifeboats (16 wooden lifeboats and four collapsible) could carry 53% of total passengers.\n",
        "* The Age of all passengers vary from about 0.17 year to 80 years with average 29.88. The mean age of survived passenger is 28.23 which on 2.39 smaller than the mean age of drowned passengers (only passengers we know survived status for). It looks like there is a slightly bigger chance to survive for younger people.\n",
        "* Exploring the title of passengers, we can see that the biggest proportion of survivors is in the \"Mrs\" group - a married woman. More than 80% drowned in the \"Mr.\" group and nobody survived among the Reverend group.\n",
        "* Most passengers don't have cabin numbers. The largest part of passengers with known cabin numbers was located on the 'C' deck and had 1st class ticket. 'C' deck is fifth by a percentage of the survivor.  \n",
        "The largest surviving rate (among passengers with known cabin numbers in training dataset) had passengers from deck 'D'. Deck A was the closest to the deck with lifeboats, but it is the last in the surviving rate.\n",
        "* The family size on board also seems to have an influence on chances for survival: there were two large families with sizes 8 and 11 and all their members from the training dataset are drowned.  We can observe that the percentage of survivors in people who have a family of 2, 3, 4 people is greater than in singles, then the percentage of survivors decreases as the family size increases.  \n",
        "* Most of the Titanic's passengers were traveling third class (709). The second class is the smallest in terms of the number of passengers. Despite the previously identified prerequisites (on average, older people are more likely to die, and in the first class, the average age is higher than in other classes. Also, passengers on deck A, which consists of 100% first class, have a large proportion of drowned passengers), the first-class has the largest number of survivors and the proportion of survivors within the class is the largest.  \n",
        "Third-class tickets had the highest number of drowned passengers, and most of the third-class passengers drowned.\n",
        "* However, most of the male passengers of the first class drowned, and the female almost all survived. In the third grade, half of the females survived.  \n",
        "There were overall more males than females on board, it is fair for each ticket class, but in the 3rd class number of males more than twice bigger than females.  \n",
        "Almost 600 male passengers traveled without family members and only about 200 females, but in usual and big families there were slightly more female passengers.  \n",
        "* Most numbers of passengers (914) were embarked in Southampton. Also, Southampton has the biggest proportion of drowned passengers. 270 passengers embarked in Cherbourg and more than 50% of them survived (in the training dataset). 123 of passengers embarked in Queenstown, the vast majority of them are 3rd class passengers.  \n",
        "\n",
        " \n",
        "If we use a naive approach and consider all the parameters separately, then young female first-class passengers with the title Mrs have a moderate number of relatives on Board, who paid a large amount for a ticket and went on Board in  Cherbourg have a better chance of survival. \n",
        "For sure, there is a relationship between variables, and survival was influenced not only by the title, or ticket, or age itself but by a combination of factors that are to some extent interrelated.   \n",
        "And It is obvious that there is no algorithm that can predict the survival rate by 100 percent based on the factors of the passenger's location on the ship or his age, since the human factor and the unpredicted emergency were involved in the rescue process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXL--I1oFb66"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ele4Q9LOE9gN"
      },
      "source": [
        "# 8. Handling missing values/ outliers/ mistakes\n",
        " I'm just applying the appropriate techniques based on the information we have found in the previous sections.\n",
        "\n",
        "## 8.1. Mistakes, outliers\n",
        "From my EDA we discovered some mistakes in data:\n",
        "* Two passengers with ids 69 and 1106 have wrong number of SibSp and Parch.\n",
        "* Passenger 631 has wrong age."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ue7y8sDrE9gO"
      },
      "source": [
        "# Passengers with wrong number of siblings and parch\n",
        "train_data.loc[train_data['PassengerId'] == 69, ['SibSp', 'Parch']] = [0,0]\n",
        "test_data.loc[test_data['PassengerId'] == 1106, ['SibSp', 'Parch']] = [0,0]\n",
        "\n",
        "# Age outlier \n",
        "train_data.loc[train_data['PassengerId'] == 631, 'Age'] = 48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fV_Ys_lE9gO"
      },
      "source": [
        "## 8.2. Missing values\n",
        "Missing values can be caused by different reasons, there are three main [mechanisms](https://www.theanalysisfactor.com/missing-data-mechanism/):  \n",
        "* **Missing Completely at Random** (MCAR) - there is no relationship between the missingness of the data and any values, observed or missing.\n",
        "* **Missing at Random** (MAR) - there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.\n",
        "* **Missing Not at Random**(MNAR) - there is a relationship between the propensity of a value to be missing and its values. \n",
        "\n",
        "Depending on reasons, why values are missing, we should act differently with data imputation. It's not easy to say for sure, what type of missing values we have, but we can do some assumptions.\n",
        "\n",
        "For example, based on the fact that among the passengers without the specified cabin, the percentage of drowned is higher, you might think that the presence of this information tells us something (MAR). Cabin information is missing from a very large number of passengers (687 NA in training dataset and 327 in test dataset). I read about the [assumptions](https://www.encyclopedia-titanica.org/cabins.html) that this information was collected from the words of survived passengers. If this is the case, then the presence or absence of data is itself important information.  \n",
        "To preserve a possible insight, we can add a separate flag that the passenger cabin is unknown. I use the cabin to determine the passenger's deck, for those who do not have the information, I will add the 'U' flag. I don't plan to use the cabin itself.  \n",
        "\n",
        "Age can be a similar situation (there are bigger percent of passengers who survived among passengers with not NA Age), so I will add a flag that age is not specified ('Age_NA') before inserting the assumed values into the age variable.  \n",
        "\n",
        "There are table of missing values:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "p7jPin1aE9gO"
      },
      "source": [
        "# check data for NA values\n",
        "train_NA = train_data.isna().sum()\n",
        "test_NA = test_data.isna().sum()\n",
        "pd.concat([train_NA, test_NA], axis=1, sort = False, keys = ['Train NA', 'Test NA'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE1Cc3WwE9gP"
      },
      "source": [
        "Using sns.heatmap we can visualise missing values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "oAPqT0gME9gP"
      },
      "source": [
        "plt.figure(figsize = (16, 7))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.heatmap(train_data.isnull(), cbar=False)\n",
        "plt.xticks(rotation = 35,     horizontalalignment='right',\n",
        "    fontweight='light'  )\n",
        "plt.title('Training dataset missing values')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.heatmap(test_data.isnull(), cbar=False)\n",
        "plt.xticks(rotation=35,     horizontalalignment='right',\n",
        "    fontweight='light'  )\n",
        "plt.title('Test dataset missing values')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvTsphXAE9gQ"
      },
      "source": [
        "### 8.2.1 Age \n",
        "First, add Age_NA variable indicates that there is no age in the original data.  \n",
        "Second, I will imput median Age calculated inside of each Pclass + Sex + Title_category group. To do so, I extracting Title from Name variable and map it on title category dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7nJhaaYGE9gQ"
      },
      "source": [
        "# Add new variable Age_NA indicates that there is no age in the original data.\n",
        "train_data.loc[train_data['Age'].isna(), 'Age_NA'] = 1     # 1 for missing Age value\n",
        "train_data.loc[train_data['Age_NA'].isna(), 'Age_NA'] = 0  # 0 if Age value is not null\n",
        "test_data.loc[test_data['Age'].isna(), 'Age_NA'] = 1       \n",
        "test_data.loc[test_data['Age_NA'].isna(), 'Age_NA'] = 0\n",
        "\n",
        "# titles categories dict\n",
        "title_dict = {  'Mr':     'Mr',\n",
        "                'Mrs':    'Mrs',\n",
        "                'Miss':   'Miss',\n",
        "                'Master': 'Master',              \n",
        "                'Ms':     'Miss',\n",
        "                'Mme':    'Mrs',\n",
        "                'Mlle':   'Miss',\n",
        "                'Capt':   'military',\n",
        "                'Col':    'military',\n",
        "                'Major':  'military',\n",
        "                'Dr':     'Dr',\n",
        "                'Rev':    'Rev',                  \n",
        "                'Sir':    'honor',\n",
        "                'the Countess': 'honor',\n",
        "                'Lady':   'honor',\n",
        "                'Jonkheer': 'honor',\n",
        "                'Don':    'honor',\n",
        "                'Dona':   'honor' }\n",
        "\n",
        "# add title variable\n",
        "train_data['Title'] = train_data['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\n",
        "test_data['Title'] = test_data['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\n",
        "\n",
        "# map titles to category\n",
        "train_data['Title_category'] = train_data['Title'].map(title_dict)\n",
        "test_data['Title_category'] = test_data['Title'].map(title_dict)\n",
        "\n",
        "# delete Title variable\n",
        "del train_data['Title']\n",
        "del test_data['Title']\n",
        "\n",
        "# Filling the missing values in Age with the medians of Sex and Pclass, Title groups\n",
        "train_data['Age'] = train_data.groupby(['Pclass', 'Sex', 'Title_category'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
        "test_data['Age'] = test_data.groupby(['Pclass', 'Sex', 'Title_category'])['Age'].apply(lambda x: x.fillna(x.median()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io1NYHwjE9gQ"
      },
      "source": [
        "### 8.2.2 Embarked\n",
        "There are two missing Embarked values. Both of the passengers are with ticket number = 113572 (and same cabin number), so they traveled together. I will imput values with mode value for same class passengers in similar fare group (77-82), because Fare should depend on class and duration of the trip (which depends on the place of boarding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ILrj6Lk5E9gR"
      },
      "source": [
        "train_data[train_data['Embarked'].isna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2jjZoUlrE9gR"
      },
      "source": [
        "mode_emb = train_data[(train_data['Fare'] > 77) & (train_data['Fare'] < 82)& (train_data['Pclass']==1)]['Embarked'].mode()\n",
        "train_data.loc[train_data['Embarked'].isna(), 'Embarked'] = mode_emb[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBxfhOPvE9gR"
      },
      "source": [
        "### 8.2.3 Fare\n",
        "Only one fare value is missed. I will fill NA with median Fare calculated inside of same  Pclass + Sex + Title_category + Parch group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CYTt2TM6E9gR"
      },
      "source": [
        "# Filling the missing values in Age with the medians of Sex and Pclass, Title groups\n",
        "test_data['Fare'] = test_data.groupby(['Pclass', 'Sex', 'Title_category', 'Parch'])['Fare'].apply(lambda x: x.fillna(x.median()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H1OQbr9E9gS"
      },
      "source": [
        "# 9. Feature generation\n",
        "From 11 variables I will generate **31 features** for each passenger.  \n",
        "We need to transform categorical variables into numbers, because most models only accept numbers as input. I will use following ways to do it:\n",
        "1. **Dummy variables** -a numerical variable used to represent subgroups of the sample in your study. Takes only the value 0 or 1 to indicate the absence or presence of some feature. This method are very suitable for nominal categorical variables, which has no intrinsic ordering to its categories.\n",
        "\n",
        "2. **Replace string values by numbers** - suitable for ordinal variables (such as passenger ticket class) or if we have only two possible values (In this dataset we can implement it to Sex, for example).\n",
        "\n",
        "Features:\n",
        "* **Deck** (8 features) - From the number of the cabin we can extract first letter, which will tell us about placement of the cabin on the ship (Deck). If there is no Cain number, the value will be 'U' - unknown. \n",
        "Using get_dummies function, I convert it to 8 dummy features with prefix 'deck'.\n",
        "* **Title_category** (8 features) - We already created 'Title_category' variable in missing imputation part.\n",
        "Using get_dummies function, I convert it to 8 dummy features with prefix 'title'.\n",
        "* **Family_size_group** (4 features) - I calculate Family_size by summarizing SibSp and Parch variables and adding 1. Then I create Family_size_group (4 possible values) based on family size. \n",
        "Using get_dummies function, I convert it to 8 dummy features with prefix 'family'.\n",
        "* **Age_NA** (1 feature) -  We created this variable during missing imputation process.\n",
        "* **Zero_fare** (1 feature) - Mark people with zero fare, since they can be related to the White Star Line.\n",
        "* **Embarked** (3 features) - Using get_dummies function, I convert 'Embarked' variable to 3 dummy features with prefix 'embarked'.\n",
        "* **Sex**  (1 feature) - since we have only 2 possible values, I conver it to binary feature - 1 if female, 0 if male.\n",
        "* **Age** (1 feature) - already numeric (continuous).\n",
        "* **Fare** (1 feature) - already numeric (continuous).\n",
        "* **Pclass** (1 feature) - already numeric (ordinal categorical variable).\n",
        "* **SibSp** (1 feature) -  since we already use size of family, I conver SibSp to binary feature - 1 if there are siblings / spouses aboard the Titanic, 0 if not.\n",
        "* **Parch** (1 feature) -  since we already use size of family, I conver Parch to binary feature - 1 if there are parents / children aboard the Titanic, 0 if not.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBRmarmqE9gS"
      },
      "source": [
        "## 9.1 Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LwMYz3gkE9gS"
      },
      "source": [
        "def feature_generator (data, train = False):\n",
        "    \n",
        "    features_data = data\n",
        "    \n",
        "    # Deck\n",
        "    # Extract deck letter from cabin number\n",
        "    features_data['deck'] = features_data['Cabin'].str.split('', expand = True)[1]\n",
        "    # If cabin is NA - deck = U\n",
        "    features_data.loc[features_data['deck'].isna(), 'deck'] = 'U'\n",
        "    # If cabin is T - change to A (see EDA)\n",
        "    features_data.loc[features_data['deck'] == 'T', 'deck'] = 'A'\n",
        "    # Create dummy variables with prefix 'deck'\n",
        "    features_data = pd.concat([features_data,\n",
        "                               pd.get_dummies(features_data['deck'], prefix = 'deck')], \n",
        "                               axis=1)\n",
        "    \n",
        "    \n",
        "    # titles dummy\n",
        "    features_data = pd.concat([features_data, \n",
        "                               pd.get_dummies(features_data['Title_category'],\n",
        "                                              prefix = 'title')], axis=1)\n",
        "\n",
        "    # family size\n",
        "    features_data['Family_size'] = features_data['SibSp'] + features_data['Parch'] + 1\n",
        "    features_data['Family_size_group'] = features_data['Family_size'].map(\n",
        "                                            lambda x: 'f_single' if x == 1 \n",
        "                                                    else ('f_usual' if 5 > x >= 2 \n",
        "                                                          else ('f_big' if 8 > x >= 5 \n",
        "                                                               else 'f_large' )))\n",
        "    features_data = pd.concat([features_data, \n",
        "                               pd.get_dummies(features_data['Family_size_group'], \n",
        "                                              prefix = 'family')], axis=1)     \n",
        "    \n",
        "    \n",
        "    # Sex to number\n",
        "    features_data['Sex'] = features_data['Sex'].map({'female': 1, 'male': 0}).astype(int)\n",
        "    \n",
        "    # embarked dummy\n",
        "    features_data = pd.concat([features_data, \n",
        "                               pd.get_dummies(features_data['Embarked'], \n",
        "                                              prefix = 'embarked')], axis=1)\n",
        "    \n",
        "    # zero fare feature\n",
        "    features_data['zero_fare'] = features_data['Fare'].map(lambda x: 1 if x == 0 else (0))\n",
        "    \n",
        "    # from numeric to categorical\n",
        "    features_data['SibSp'] = features_data['SibSp'].map(lambda x: 1 if x > 0 else (0))\n",
        "    features_data['Parch'] = features_data['Parch'].map(lambda x: 1 if x > 0 else (0))\n",
        "    \n",
        "    # delete variables we are not going to use anymore\n",
        "    del features_data['PassengerId']\n",
        "    del features_data['Ticket']\n",
        "    del features_data['Cabin']\n",
        "    del features_data['deck']    \n",
        "    del features_data['Title_category']\n",
        "    del features_data['Name']\n",
        "    del features_data['Family_size']\n",
        "    del features_data['Family_size_group'] \n",
        "    del features_data['Embarked']    \n",
        "    \n",
        "    return features_data     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUkkqpFkE9gT"
      },
      "source": [
        "## 9.2 Generation\n",
        "Generte features for training and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gNLRLOXCE9gT"
      },
      "source": [
        "# Extract target variable (label) from training dataset\n",
        "all_train_label = train_data['Survived']\n",
        "del train_data['Survived']\n",
        "\n",
        "# Generate features from training dataset\n",
        "all_train_features = feature_generator(train_data)\n",
        "# Generate features from test dataset\n",
        "all_test_features = feature_generator(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INEVWufwE9gT"
      },
      "source": [
        "## 9.3 Correlation\n",
        "Correlation refers to how close two variables are to having a linear relationship with each other. Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fDErkAQIE9gT"
      },
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "cor = all_train_features.corr()\n",
        "sns.heatmap(cor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQJN1GgdE9gT"
      },
      "source": [
        "Аrom the graph, we can see that some features have correlation (for example, here is a title Mr and the Sex of the passenger, the size of the family - single and the presence of siblings, fare and class). However, I decided to leave all the attributes to see how the models will sort them by importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTo8lR_sE9gc"
      },
      "source": [
        "# 10. Models\n",
        "Who would have survived the Titanic disaster - a classification problem. Base on input (features) we need to determine to which 'class' the passenger belongs - survived or not. In this notebook I will explain, tune and compare three different models, to solve this problem:\n",
        "* Logistic regression\n",
        "* Random forest\n",
        "* XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjJKf7DAE9gc"
      },
      "source": [
        "## 10.1 Explore and tune models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up4AvMxgE9gc"
      },
      "source": [
        "### 10.1.1 How to tune\n",
        "For tuning models I will use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html ) from sklearn. Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model.   \n",
        "GridSearchCV checks all combinations of the proposed parameters and returns the best configuration based on scoring parameter.  \n",
        "GridSearch**CV** uses cross validation technique. Depending on cv parameter value, it splits data on n folders and in each iteration uses one folder as a validation data (unseen data), switching folders. This allowes to use all the data to test the model, and evaluate the model on different data combinations to find the average value.\n",
        "\n",
        "\n",
        "GridSearchCV parameters:\n",
        "* **estimator**: estimator object you created\n",
        "* **params_grid**: the dictionary object that holds the hyperparameters you want to try\n",
        "* **scoring**: evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric\n",
        "* **cv**: number of cross-validation you have to try for each selected set of hyperparameters\n",
        "* **verbose**: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV\n",
        "* **n_jobs**: number of processes you wish to run in parallel for this task if it -1 it will use all available processors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOVMPT6CE9gd"
      },
      "source": [
        "### 10.1.2 Logistic regression\n",
        "#### 10.1.2.1 About Logistic regression\n",
        "[Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) is used to predict the probability of an event occurring by comparing it with the logistic curve. This regression returns the response as the probability of a binary event (Survived or not).\n",
        "\n",
        "#### 10.1.2.2 Tune\n",
        "Hyperparameters I will tune:\n",
        "* **C** - regularization parameter = 1/λ, where λ controls the trade-off between allowing the model to increase it's complexity. Small values of C increases the regularization strength which will create simple models which underfit the data. \n",
        "* **solver** - Algorithm to use in the optimization problem.\n",
        "* **class_weight** - Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ujzZ6qvaE9gd"
      },
      "source": [
        "# set model. max_iter - Maximum number of iterations taken for the solvers to converge.\n",
        "lg_model = LogisticRegression(random_state = 64, max_iter = 1000)\n",
        "\n",
        "# set parameters values we are going to check\n",
        "optimization_dict = {'class_weight':['balanced', None],\n",
        "                     'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "                     'C': [0.01, 0.05, 0.07, 0.1, 0.5, 1, 2, 4, 5, 10, 15, 20]\n",
        "                     }\n",
        "# set GridSearchCV parameters\n",
        "model = GridSearchCV(lg_model, optimization_dict, \n",
        "                     scoring='accuracy', n_jobs = -1, cv = 10)\n",
        "\n",
        "# use training features\n",
        "model.fit(all_train_features, all_train_label)\n",
        "\n",
        "# print result\n",
        "print(model.best_score_)\n",
        "print(model.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "k9ERB49JE9gd"
      },
      "source": [
        "# set best parameters to the model\n",
        "lg_tuned_model =  LogisticRegression(solver = 'newton-cg',\n",
        "                                     C = 0.5,\n",
        "                                     random_state = 64,\n",
        "                                     n_jobs = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX-_QKWSE9ge"
      },
      "source": [
        "#### 10.1.2.3 Feature importance\n",
        "For better understanding our data and how our model works, now we can calculate and visualize feature importance!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YGNyYbroE9ge"
      },
      "source": [
        "# train our model with training data\n",
        "lg_tuned_model.fit(all_train_features, all_train_label)\n",
        "\n",
        "# calculate importances based on coefficients.\n",
        "importances = abs(lg_tuned_model.coef_[0])\n",
        "importances = 100.0 * (importances / importances.max())\n",
        "# sort \n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Rearrange feature names so they match the sorted feature importances\n",
        "names = [all_train_features.columns[i] for i in indices]\n",
        "\n",
        "# visualize\n",
        "plt.figure(figsize = (12, 5))\n",
        "sns.set_style(\"whitegrid\")\n",
        "chart = sns.barplot(x = names, y = importances[indices])\n",
        "plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light'  \n",
        ")\n",
        "plt.title('Logistic regression. Feature importance')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-jRBDujE9ge"
      },
      "source": [
        "Now, we can go back and compare our assumptions with this graph! As we expected, Sex is very important parameter to make prediction of survival. As well as titles Master an Mr, family size and class of passenger. \n",
        "The least important feature for Logistic regression is Fare, probably because it is correlated with features above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-X9CD9KE9ge"
      },
      "source": [
        "### 10.1.3 Random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsG9TDXuE9ge"
      },
      "source": [
        "#### 10.1.3.1 What is random forest\n",
        "To understand the random forest, first, we need to understand other Machine Learning algorithm - Decision Trees.  \n",
        "\n",
        "**Decision trees**, in simple words, it is a tree-like model of decisions. A decision tree starts with a single node (whole population), then branches by condition (feature) into possible outcomes. These outcomes, in turn, lead to nodes, which branch off into other possibilities or to leaf node, which represents a class label.  \n",
        "We choose the next feature for splitting the data based on information gain value in order to decrease in entropy after a data-set is split on an attribute or gini index. \n",
        "\n",
        "**Random forest** consists of a number of individual decision trees that operate as an ensemble. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. Random forest makes decision by taking the majority vote.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaXlWwYlE9ge"
      },
      "source": [
        "#### 10.1.3.2 Tune\n",
        "Hyperparameters I will tune:\n",
        "* **criterion** - the function to measure the quality of a split. It can either be “gini” or “entropy”. “gini” uses the Gini impurity while “entropy” makes the split based on the information gain. \n",
        "* **n_estimators** - the number of trees in the forest.\n",
        "* **max_depth** - the maximum depth of the tree (the length of the longest path from the tree root to a leaf).  \n",
        "    If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.  \n",
        "    The deeper trees, the more complex your model will become. But if this parameter is too high, then the decision tree might simply overfit the training data without capturing useful patterns as we would like and will cause bigger test error.\n",
        "* **min_samples_split** - the minimum number of samples required to split an internal node.  \n",
        "* **min_samples_leaf** - the minimum number of samples required to be at a leaf node.\n",
        "\n",
        "Additional hyperparameters I will set:\n",
        "* **random_state** - controls both the randomness of the bootstrapping of the samples used when building trees and the sampling of the features to consider when looking for the best split at each node. \n",
        "* **n_jobs** - The number of jobs to run in parallel. fit, predict, decision_path and apply are all parallelized over the trees. -1 means using all processors. \n",
        "* **oob_score** - to use or not out-of-bag samples to estimate the generalization accuracy.\n",
        "    Each of decision tree is trained separately on bootstrap samples (each tree has different training sample). Out of Bag sample - examples not included in the training sample. So we can use it to calculate accuracy.\n",
        "* **verbose** - Controls the verbosity when fitting and predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Vdx0ktH4E9gf"
      },
      "source": [
        "# set model\n",
        "rf_model = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 64)\n",
        "# create a dictionary of parameters values we want to try\n",
        "optimization_dict = {'criterion':['gini', 'entropy'],\n",
        "                     'n_estimators': [100, 500, 1000, 1700],\n",
        "                     'max_depth': [7, 10, 11, 12],\n",
        "                     'min_samples_split': [6, 7, 8, 10],\n",
        "                     'min_samples_leaf': [3, 4, 5]\n",
        "                     }\n",
        "\n",
        "# set GridSearchCV parameters\n",
        "model = GridSearchCV(rf_model, optimization_dict, \n",
        "                     scoring='accuracy', verbose = 1, n_jobs = -1, cv = 5)\n",
        "\n",
        "# use training data\n",
        "model.fit(all_train_features, all_train_label)\n",
        "\n",
        "# print best score and best parameters combination\n",
        "print(model.best_score_)\n",
        "print(model.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U0BTxpFhE9gf"
      },
      "source": [
        "# set best parameters to the model\n",
        "rf_tuned_model =  RandomForestClassifier(criterion = 'gini',\n",
        "                                       n_estimators = 100,\n",
        "                                       max_depth = 12,\n",
        "                                       min_samples_split = 6,\n",
        "                                       min_samples_leaf = 4,\n",
        "                                       max_features = 'auto',\n",
        "                                       oob_score = True,\n",
        "                                       random_state = 64,\n",
        "                                       n_jobs = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLFcVFSrE9gf"
      },
      "source": [
        "#### 10.1.3.3 Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ewWW_2gtE9gf"
      },
      "source": [
        "# train model using training dataset\n",
        "rf_tuned_model.fit(all_train_features, all_train_label)\n",
        "\n",
        "# Calculate feature importances\n",
        "importances = rf_tuned_model.feature_importances_\n",
        "\n",
        "# Visualize Feature Importance\n",
        "# Sort feature importances in descending order\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Rearrange feature names so they match the sorted feature importances\n",
        "names = [all_train_features.columns[i] for i in indices]\n",
        "\n",
        "plt.figure(figsize = (12, 5))\n",
        "sns.set_style(\"whitegrid\")\n",
        "chart = sns.barplot(x = names, y=importances[indices])\n",
        "plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light'  \n",
        ")\n",
        "plt.title('Random forest. Feature importance')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhTfN0zzE9gf"
      },
      "source": [
        "* There a we can see different peacture - Fare feature is 3rd by importancy for the model.  \n",
        "* The most important - presence of title Mr. As we remember from EDA , Mr is the most popular title, and the percentage of drownings in this category is almost the largest, so this feature can split the dataand decrease in entropy after a data-set is split.\n",
        "* Sex is second by its importancy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yrhU0pQE9gg"
      },
      "source": [
        "### 10.1.4 XGBoost\n",
        "#### 10.1.4.1 XGBoost - e**X**treme **G**radient **B**oosting. \n",
        "\n",
        "[XGBoost](https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7) algorithm can be based on decision trees as well, but uses a different teqnicue to \"combine them\" -  gradient boosting. Boosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. Rather than training all of the models in isolation of one another, boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones. Models are added sequentially until no further improvements can be made.  \n",
        "The advantage of this iterative approach is that the new models being added are focused on correcting the mistakes which were caused by other models.  \n",
        "[XGBoost documentation.](https://xgboost.readthedocs.io/)\n",
        "\n",
        "If you are interested, I highly recommend [this \"Intermediate Machine Learning\"](https://www.kaggle.com/learn/intermediate-machine-learning) course from @alexisbcook, where you can find information about [xgboost](https://www.kaggle.com/alexisbcook/xgboost), and more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTZdjekcE9gg"
      },
      "source": [
        "#### 10.1.4.2 Tune  \n",
        "Hyperparameters:\n",
        "* **booster** - Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions. I will use default - gbtree.\n",
        "* **random_state** - Random number seed.\n",
        "* **n_estimators** - Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
        "* **max_depth** – Maximum tree depth for base learners. Increasing this value will make the model more complex and more likely to overfit.\n",
        "* **learning_rate** - Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
        "* **gamma (min_split_loss)** -  Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
        "* **min_child_weight** – Minimum sum of instance weight(hessian) needed in a child.\n",
        "* **subsample** - Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8YhwhHPUE9gg"
      },
      "source": [
        "# set model\n",
        "xgb_model = XGBClassifier(random_state = 64)\n",
        "# create a dictionary of parameters values we want to try\n",
        "optimization_dict = {'n_estimators': [200, 1000, 1700, 2000],\n",
        "                     'max_depth': [4, 6, 8, 10],\n",
        "                     'learning_rate': [0.001, 0.01, 0.1, 0.5],\n",
        "                     'gamma': [0, 1, 5],\n",
        "                     'min_child_weight':[3, 6, 10],\n",
        "                     'subsample': [0.5, 0.8, 0.9]\n",
        "                     }\n",
        "# set GridSearchCV parameters\n",
        "model = GridSearchCV(xgb_model, optimization_dict, \n",
        "                     scoring='accuracy', verbose = 1, n_jobs = -1, cv = 5)\n",
        "\n",
        "# use training data\n",
        "model.fit(all_train_features, all_train_label)\n",
        "print(model.best_score_)\n",
        "print(model.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P7Exw13nE9gg"
      },
      "source": [
        "# set model with best parameters\n",
        "xgb_tuned_model =  XGBClassifier(n_estimators = 200,\n",
        "                               max_depth = 8,\n",
        "                               learning_rate = 0.5,\n",
        "                               gamma = 1,\n",
        "                               min_child_weight = 6,\n",
        "                               subsample = 0.9,\n",
        "                               random_state = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz0_4Yr2E9gg"
      },
      "source": [
        "#### 10.4.1.3 Feature impotance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jN0JUSGrE9gh"
      },
      "source": [
        "# train model with training dataset\n",
        "xgb_tuned_model.fit(all_train_features, all_train_label)\n",
        "\n",
        "# Calculate feature importances\n",
        "importances = xgb_tuned_model.feature_importances_\n",
        "\n",
        "# Visualize Feature Importance\n",
        "# Sort feature importances in descending order\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Rearrange feature names so they match the sorted feature importances\n",
        "names = [all_train_features.columns[i] for i in indices]\n",
        "\n",
        "plt.figure(figsize = (12, 5))\n",
        "sns.set_style(\"whitegrid\")\n",
        "chart = sns.barplot(x = names, y=importances[indices])\n",
        "plt.xticks(rotation=45, horizontalalignment='right', fontweight='light')\n",
        "plt.title('XGBoost. Feature importance')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic7lcerGE9gh"
      },
      "source": [
        "* title Mr. is the most important feature to this model, similar to previous one\n",
        "* Sex  - takes only fouth position\n",
        "* Unknow deck (unknown cabin number) in the top-features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yxky2ZVE9gh"
      },
      "source": [
        "## 10.2 Compare models\n",
        "To compare models, I will use cross-validation technique as well. To do it, I will use model_selection.KFold to split data on 'folders'\n",
        "and model_selection.cross_val_score to get accuracy scores from each iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "R-h0t2whE9gh"
      },
      "source": [
        "models = []\n",
        "# add our tuned models into list\n",
        "models.append(('Logistic Regression', lg_tuned_model))\n",
        "models.append(('Random Forest', rf_tuned_model))\n",
        "models.append(('XGBoost', xgb_tuned_model))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "# evaluate each model in turn\n",
        "for name, model in models:\n",
        "    kfold = model_selection.KFold(n_splits=10, shuffle = True, random_state = 64)\n",
        "    cv_results = model_selection.cross_val_score(model, all_train_features, \n",
        "                                                 all_train_label, \n",
        "                                                 cv = 10, scoring = 'accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    # print mean accuracy and standard deviation\n",
        "    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tlIzdBQRE9gh"
      },
      "source": [
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.boxplot(results)\n",
        "plt.title('Algorithm Comparison')\n",
        "plt.xticks([1,2,3], names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}